{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from qcnn import qcnn\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_openml\n",
    "from jax.numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = lambda v: v/norm(v)\n",
    "\n",
    "def convert_to_qcnn_data(data):\n",
    "    n = data.shape[0]\n",
    "    return jnp.array([normalize(jnp.array(Image.fromarray(data[j].reshape(28,28)).resize((16,16))).reshape((256))) for j in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wfang/Robustness/qcnn/qcnn/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "digits = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0, d1 = '1', '8'\n",
    "ind0, ind1 = digits.target == d0, digits.target == d1\n",
    "x0, x1 = digits.data[ind0].to_numpy(), digits.data[ind1].to_numpy()\n",
    "y0, y1 = (digits.target[ind0] == d0).to_numpy(), (digits.target[ind1] == d0).to_numpy()\n",
    "\n",
    "n_train = 500\n",
    "n_all = 700\n",
    "\n",
    "ind0, ind1 = np.random.permutation(x0.shape[0])[:n_all], np.random.permutation(x1.shape[0])[:n_all]\n",
    "\n",
    "x_train = convert_to_qcnn_data(np.vstack((x0[ind0[:n_train]], x1[ind1[:n_train]])))\n",
    "y_train = jnp.array(np.hstack((y0[ind0[:n_train]], y1[ind1[:n_train]])))\n",
    "x_test = convert_to_qcnn_data(np.vstack((x0[ind0[n_train:n_all]], x1[ind1[n_train:n_all]])))\n",
    "y_test = jnp.array(np.hstack((y0[ind0[n_train:n_all]], y1[ind1[n_train:n_all]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = qcnn(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 13.10 sec\n",
      "loss:  0.23453037\n",
      "train accuracy:  0.592\n",
      "test accuracy:  0.59499997\n",
      "Epoch 1 in 3.85 sec\n",
      "loss:  0.22779185\n",
      "train accuracy:  0.61700004\n",
      "test accuracy:  0.635\n",
      "Epoch 2 in 3.81 sec\n",
      "loss:  0.22173817\n",
      "train accuracy:  0.669\n",
      "test accuracy:  0.68\n",
      "Epoch 3 in 3.90 sec\n",
      "loss:  0.216347\n",
      "train accuracy:  0.72200006\n",
      "test accuracy:  0.72499996\n",
      "Epoch 4 in 4.51 sec\n",
      "loss:  0.21153247\n",
      "train accuracy:  0.753\n",
      "test accuracy:  0.7525\n",
      "Epoch 5 in 4.88 sec\n",
      "loss:  0.20716572\n",
      "train accuracy:  0.78900003\n",
      "test accuracy:  0.79249996\n",
      "Epoch 6 in 4.23 sec\n",
      "loss:  0.20313384\n",
      "train accuracy:  0.804\n",
      "test accuracy:  0.8075\n",
      "Epoch 7 in 3.88 sec\n",
      "loss:  0.19935578\n",
      "train accuracy:  0.81500006\n",
      "test accuracy:  0.8225\n",
      "Epoch 8 in 4.37 sec\n",
      "loss:  0.19578083\n",
      "train accuracy:  0.82000005\n",
      "test accuracy:  0.84499997\n",
      "Epoch 9 in 5.11 sec\n",
      "loss:  0.19238216\n",
      "train accuracy:  0.82600003\n",
      "test accuracy:  0.85249996\n",
      "Epoch 10 in 4.07 sec\n",
      "loss:  0.18914787\n",
      "train accuracy:  0.836\n",
      "test accuracy:  0.85749996\n",
      "Epoch 11 in 4.13 sec\n",
      "loss:  0.18607418\n",
      "train accuracy:  0.841\n",
      "test accuracy:  0.86249995\n",
      "Epoch 12 in 4.05 sec\n",
      "loss:  0.18316224\n",
      "train accuracy:  0.845\n",
      "test accuracy:  0.8675\n",
      "Epoch 13 in 4.36 sec\n",
      "loss:  0.180414\n",
      "train accuracy:  0.84700006\n",
      "test accuracy:  0.8725\n",
      "Epoch 14 in 4.20 sec\n",
      "loss:  0.17782995\n",
      "train accuracy:  0.851\n",
      "test accuracy:  0.8725\n",
      "Epoch 15 in 4.28 sec\n",
      "loss:  0.17540409\n",
      "train accuracy:  0.851\n",
      "test accuracy:  0.8775\n",
      "Epoch 16 in 4.05 sec\n",
      "loss:  0.17312525\n",
      "train accuracy:  0.85200006\n",
      "test accuracy:  0.8825\n",
      "Epoch 17 in 4.15 sec\n",
      "loss:  0.17098068\n",
      "train accuracy:  0.855\n",
      "test accuracy:  0.885\n",
      "Epoch 18 in 4.14 sec\n",
      "loss:  0.16895951\n",
      "train accuracy:  0.855\n",
      "test accuracy:  0.8875\n",
      "Epoch 19 in 4.14 sec\n",
      "loss:  0.1670526\n",
      "train accuracy:  0.85600007\n",
      "test accuracy:  0.8875\n",
      "Epoch 20 in 4.03 sec\n",
      "loss:  0.16525367\n",
      "train accuracy:  0.85600007\n",
      "test accuracy:  0.8875\n",
      "Epoch 21 in 4.25 sec\n",
      "loss:  0.16355836\n",
      "train accuracy:  0.85800004\n",
      "test accuracy:  0.8875\n",
      "Epoch 22 in 3.99 sec\n",
      "loss:  0.16196267\n",
      "train accuracy:  0.86100006\n",
      "test accuracy:  0.8875\n",
      "Epoch 23 in 4.21 sec\n",
      "loss:  0.1604614\n",
      "train accuracy:  0.86200005\n",
      "test accuracy:  0.89\n",
      "Epoch 24 in 4.18 sec\n",
      "loss:  0.15904671\n",
      "train accuracy:  0.86300004\n",
      "test accuracy:  0.8925\n",
      "Epoch 25 in 4.35 sec\n",
      "loss:  0.15770704\n",
      "train accuracy:  0.86600006\n",
      "test accuracy:  0.89\n",
      "Epoch 26 in 4.06 sec\n",
      "loss:  0.15642864\n",
      "train accuracy:  0.869\n",
      "test accuracy:  0.89\n",
      "Epoch 27 in 4.12 sec\n",
      "loss:  0.15519698\n",
      "train accuracy:  0.87000006\n",
      "test accuracy:  0.8925\n",
      "Epoch 28 in 4.11 sec\n",
      "loss:  0.15400086\n",
      "train accuracy:  0.87100005\n",
      "test accuracy:  0.8925\n",
      "Epoch 29 in 4.05 sec\n",
      "loss:  0.15283142\n",
      "train accuracy:  0.87200004\n",
      "test accuracy:  0.8925\n",
      "Epoch 30 in 4.02 sec\n",
      "loss:  0.15168452\n",
      "train accuracy:  0.873\n",
      "test accuracy:  0.8975\n",
      "Epoch 31 in 4.08 sec\n",
      "loss:  0.1505596\n",
      "train accuracy:  0.87700003\n",
      "test accuracy:  0.8975\n",
      "Epoch 32 in 4.15 sec\n",
      "loss:  0.14945804\n",
      "train accuracy:  0.88100004\n",
      "test accuracy:  0.8975\n",
      "Epoch 33 in 4.08 sec\n",
      "loss:  0.14838323\n",
      "train accuracy:  0.88100004\n",
      "test accuracy:  0.9025\n",
      "Epoch 34 in 4.13 sec\n",
      "loss:  0.1473384\n",
      "train accuracy:  0.8880001\n",
      "test accuracy:  0.90999997\n",
      "Epoch 35 in 4.09 sec\n",
      "loss:  0.14632584\n",
      "train accuracy:  0.89000005\n",
      "test accuracy:  0.91499996\n",
      "Epoch 36 in 4.16 sec\n",
      "loss:  0.14534657\n",
      "train accuracy:  0.89400005\n",
      "test accuracy:  0.91499996\n",
      "Epoch 37 in 4.29 sec\n",
      "loss:  0.1444004\n",
      "train accuracy:  0.89900005\n",
      "test accuracy:  0.91999996\n",
      "Epoch 38 in 4.19 sec\n",
      "loss:  0.14348477\n",
      "train accuracy:  0.90300006\n",
      "test accuracy:  0.91999996\n",
      "Epoch 39 in 4.03 sec\n",
      "loss:  0.14259653\n",
      "train accuracy:  0.90400004\n",
      "test accuracy:  0.92499995\n",
      "Epoch 40 in 4.11 sec\n",
      "loss:  0.14173189\n",
      "train accuracy:  0.90800005\n",
      "test accuracy:  0.92499995\n",
      "Epoch 41 in 4.06 sec\n",
      "loss:  0.14088681\n",
      "train accuracy:  0.91700006\n",
      "test accuracy:  0.935\n",
      "Epoch 42 in 4.06 sec\n",
      "loss:  0.1400586\n",
      "train accuracy:  0.92200005\n",
      "test accuracy:  0.9425\n",
      "Epoch 43 in 4.02 sec\n",
      "loss:  0.1392455\n",
      "train accuracy:  0.92600006\n",
      "test accuracy:  0.945\n",
      "Epoch 44 in 4.09 sec\n",
      "loss:  0.13844638\n",
      "train accuracy:  0.92800003\n",
      "test accuracy:  0.95\n",
      "Epoch 45 in 4.10 sec\n",
      "loss:  0.13766178\n",
      "train accuracy:  0.93000007\n",
      "test accuracy:  0.9575\n",
      "Epoch 46 in 4.20 sec\n",
      "loss:  0.13689138\n",
      "train accuracy:  0.93100005\n",
      "test accuracy:  0.9525\n",
      "Epoch 47 in 4.02 sec\n",
      "loss:  0.13613456\n",
      "train accuracy:  0.93500006\n",
      "test accuracy:  0.9525\n",
      "Epoch 48 in 4.01 sec\n",
      "loss:  0.13538872\n",
      "train accuracy:  0.93600005\n",
      "test accuracy:  0.9525\n",
      "Epoch 49 in 3.71 sec\n",
      "loss:  0.13465051\n",
      "train accuracy:  0.94400007\n",
      "test accuracy:  0.9575\n",
      "Epoch 50 in 3.90 sec\n",
      "loss:  0.13391544\n",
      "train accuracy:  0.94500005\n",
      "test accuracy:  0.9575\n",
      "Epoch 51 in 3.73 sec\n",
      "loss:  0.13317926\n",
      "train accuracy:  0.947\n",
      "test accuracy:  0.96\n",
      "Epoch 52 in 3.67 sec\n",
      "loss:  0.13243769\n",
      "train accuracy:  0.947\n",
      "test accuracy:  0.965\n",
      "Epoch 53 in 3.58 sec\n",
      "loss:  0.13168931\n",
      "train accuracy:  0.947\n",
      "test accuracy:  0.9675\n",
      "Epoch 54 in 3.61 sec\n",
      "loss:  0.13093261\n",
      "train accuracy:  0.9480001\n",
      "test accuracy:  0.96999997\n",
      "Epoch 55 in 3.76 sec\n",
      "loss:  0.13016926\n",
      "train accuracy:  0.95000005\n",
      "test accuracy:  0.96999997\n",
      "Epoch 56 in 3.63 sec\n",
      "loss:  0.12940076\n",
      "train accuracy:  0.952\n",
      "test accuracy:  0.9675\n",
      "Epoch 57 in 3.79 sec\n",
      "loss:  0.12862991\n",
      "train accuracy:  0.95100003\n",
      "test accuracy:  0.9675\n",
      "Epoch 58 in 3.80 sec\n",
      "loss:  0.12785889\n",
      "train accuracy:  0.95500004\n",
      "test accuracy:  0.96999997\n",
      "Epoch 59 in 3.81 sec\n",
      "loss:  0.12709121\n",
      "train accuracy:  0.957\n",
      "test accuracy:  0.96999997\n",
      "Epoch 60 in 3.77 sec\n",
      "loss:  0.12632968\n",
      "train accuracy:  0.957\n",
      "test accuracy:  0.96999997\n",
      "Epoch 61 in 3.94 sec\n",
      "loss:  0.12557632\n",
      "train accuracy:  0.957\n",
      "test accuracy:  0.96999997\n",
      "Epoch 62 in 3.96 sec\n",
      "loss:  0.12483261\n",
      "train accuracy:  0.95800006\n",
      "test accuracy:  0.96999997\n",
      "Epoch 63 in 4.26 sec\n",
      "loss:  0.12410002\n",
      "train accuracy:  0.95800006\n",
      "test accuracy:  0.96999997\n",
      "Epoch 64 in 4.25 sec\n",
      "loss:  0.12337958\n",
      "train accuracy:  0.95900005\n",
      "test accuracy:  0.97249997\n",
      "Epoch 65 in 4.36 sec\n",
      "loss:  0.12267368\n",
      "train accuracy:  0.95900005\n",
      "test accuracy:  0.97499996\n",
      "Epoch 66 in 4.28 sec\n",
      "loss:  0.121984355\n",
      "train accuracy:  0.95900005\n",
      "test accuracy:  0.97499996\n",
      "Epoch 67 in 4.27 sec\n",
      "loss:  0.12131405\n",
      "train accuracy:  0.96000004\n",
      "test accuracy:  0.97499996\n",
      "Epoch 68 in 4.29 sec\n",
      "loss:  0.12066471\n",
      "train accuracy:  0.96000004\n",
      "test accuracy:  0.97499996\n",
      "Epoch 69 in 4.02 sec\n",
      "loss:  0.120038256\n",
      "train accuracy:  0.95900005\n",
      "test accuracy:  0.97499996\n",
      "Epoch 70 in 4.35 sec\n",
      "loss:  0.119435616\n",
      "train accuracy:  0.95900005\n",
      "test accuracy:  0.97499996\n",
      "Epoch 71 in 4.14 sec\n",
      "loss:  0.11885729\n",
      "train accuracy:  0.95900005\n",
      "test accuracy:  0.97499996\n",
      "Epoch 72 in 4.02 sec\n",
      "loss:  0.118303\n",
      "train accuracy:  0.95900005\n",
      "test accuracy:  0.97749996\n",
      "Epoch 73 in 4.14 sec\n",
      "loss:  0.11777212\n",
      "train accuracy:  0.95900005\n",
      "test accuracy:  0.97749996\n",
      "Epoch 74 in 4.13 sec\n",
      "loss:  0.11726323\n",
      "train accuracy:  0.96000004\n",
      "test accuracy:  0.97749996\n",
      "Epoch 75 in 5.04 sec\n",
      "loss:  0.11677365\n",
      "train accuracy:  0.96000004\n",
      "test accuracy:  0.97749996\n",
      "Epoch 76 in 3.85 sec\n",
      "loss:  0.11630222\n",
      "train accuracy:  0.961\n",
      "test accuracy:  0.97749996\n",
      "Epoch 77 in 3.65 sec\n",
      "loss:  0.115847066\n",
      "train accuracy:  0.961\n",
      "test accuracy:  0.97749996\n",
      "Epoch 78 in 3.63 sec\n",
      "loss:  0.11540614\n",
      "train accuracy:  0.96000004\n",
      "test accuracy:  0.97749996\n",
      "Epoch 79 in 4.81 sec\n",
      "loss:  0.11497806\n",
      "train accuracy:  0.961\n",
      "test accuracy:  0.97749996\n",
      "Epoch 80 in 5.10 sec\n",
      "loss:  0.11456195\n",
      "train accuracy:  0.9620001\n",
      "test accuracy:  0.97749996\n",
      "Epoch 81 in 4.42 sec\n",
      "loss:  0.114157274\n",
      "train accuracy:  0.9620001\n",
      "test accuracy:  0.97749996\n",
      "Epoch 82 in 4.24 sec\n",
      "loss:  0.11376384\n",
      "train accuracy:  0.9620001\n",
      "test accuracy:  0.97749996\n",
      "Epoch 83 in 4.33 sec\n",
      "loss:  0.11338206\n",
      "train accuracy:  0.96300006\n",
      "test accuracy:  0.97749996\n",
      "Epoch 84 in 4.07 sec\n",
      "loss:  0.113011844\n",
      "train accuracy:  0.9620001\n",
      "test accuracy:  0.97749996\n",
      "Epoch 85 in 4.23 sec\n",
      "loss:  0.11265356\n",
      "train accuracy:  0.96300006\n",
      "test accuracy:  0.97749996\n",
      "Epoch 86 in 3.99 sec\n",
      "loss:  0.112307504\n",
      "train accuracy:  0.96300006\n",
      "test accuracy:  0.97749996\n",
      "Epoch 87 in 3.87 sec\n",
      "loss:  0.111973874\n",
      "train accuracy:  0.96300006\n",
      "test accuracy:  0.97749996\n",
      "Epoch 88 in 3.80 sec\n",
      "loss:  0.11165255\n",
      "train accuracy:  0.96300006\n",
      "test accuracy:  0.97749996\n",
      "Epoch 89 in 3.71 sec\n",
      "loss:  0.111343235\n",
      "train accuracy:  0.96400005\n",
      "test accuracy:  0.97749996\n",
      "Epoch 90 in 3.52 sec\n",
      "loss:  0.11104621\n",
      "train accuracy:  0.96400005\n",
      "test accuracy:  0.97749996\n",
      "Epoch 91 in 3.56 sec\n",
      "loss:  0.11076108\n",
      "train accuracy:  0.96500003\n",
      "test accuracy:  0.97749996\n",
      "Epoch 92 in 3.44 sec\n",
      "loss:  0.11048752\n",
      "train accuracy:  0.96500003\n",
      "test accuracy:  0.97749996\n",
      "Epoch 93 in 3.53 sec\n",
      "loss:  0.11022518\n",
      "train accuracy:  0.96500003\n",
      "test accuracy:  0.97749996\n",
      "Epoch 94 in 3.75 sec\n",
      "loss:  0.10997397\n",
      "train accuracy:  0.96500003\n",
      "test accuracy:  0.97749996\n",
      "Epoch 95 in 3.68 sec\n",
      "loss:  0.10973332\n",
      "train accuracy:  0.96500003\n",
      "test accuracy:  0.97749996\n",
      "Epoch 96 in 4.18 sec\n",
      "loss:  0.109502584\n",
      "train accuracy:  0.96500003\n",
      "test accuracy:  0.97749996\n",
      "Epoch 97 in 3.83 sec\n",
      "loss:  0.10928072\n",
      "train accuracy:  0.966\n",
      "test accuracy:  0.97749996\n",
      "Epoch 98 in 3.90 sec\n",
      "loss:  0.10906702\n",
      "train accuracy:  0.966\n",
      "test accuracy:  0.97749996\n",
      "Epoch 99 in 4.05 sec\n",
      "loss:  0.108860314\n",
      "train accuracy:  0.966\n",
      "test accuracy:  0.97749996\n"
     ]
    }
   ],
   "source": [
    "model.train(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENQASM 2.0;\n",
      "include \"qelib1.inc\";\n",
      "qreg q[8];\n",
      "creg c[1];\n",
      "rx(12.611748695373535) q[0];\n",
      "rz(9.912858009338379) q[0];\n",
      "rx(3.3747339248657227) q[0];\n",
      "rx(0.46360480785369873) q[1];\n",
      "rz(10.170737266540527) q[1];\n",
      "rx(9.362000465393066) q[1];\n",
      "crz(7.456787109375) q[0], q[1];\n",
      "rx(0.9328164458274841) q[2];\n",
      "rz(-0.05429160222411156) q[2];\n",
      "rx(10.249898910522461) q[2];\n",
      "rx(10.824579238891602) q[3];\n",
      "rz(9.4149169921875) q[3];\n",
      "rx(9.110272407531738) q[3];\n",
      "crz(4.0343427658081055) q[2], q[3];\n",
      "rx(8.385578155517578) q[4];\n",
      "rz(4.1696929931640625) q[4];\n",
      "rx(6.962714195251465) q[4];\n",
      "rx(12.243192672729492) q[5];\n",
      "rz(12.670632362365723) q[5];\n",
      "rx(8.944207191467285) q[5];\n",
      "crz(3.681565999984741) q[4], q[5];\n",
      "rx(7.052025318145752) q[6];\n",
      "rz(8.577873229980469) q[6];\n",
      "rx(6.407253742218018) q[6];\n",
      "rx(5.134056568145752) q[7];\n",
      "rz(10.672517776489258) q[7];\n",
      "rx(10.241750717163086) q[7];\n",
      "crz(3.8025803565979004) q[6], q[7];\n",
      "rx(9.867083549499512) q[1];\n",
      "rz(5.151313304901123) q[1];\n",
      "rx(5.327807426452637) q[1];\n",
      "rx(1.639193058013916) q[2];\n",
      "rz(2.487518548965454) q[2];\n",
      "rx(-0.036469683051109314) q[2];\n",
      "crz(8.333980560302734) q[2], q[1];\n",
      "rx(8.431339263916016) q[3];\n",
      "rz(6.026973247528076) q[3];\n",
      "rx(0.4197891354560852) q[3];\n",
      "rx(0.6581267714500427) q[4];\n",
      "rz(5.3237199783325195) q[4];\n",
      "rx(7.508982181549072) q[4];\n",
      "crz(5.353586673736572) q[4], q[3];\n",
      "rx(9.952875137329102) q[5];\n",
      "rz(7.498066425323486) q[5];\n",
      "rx(9.591459274291992) q[5];\n",
      "rx(9.285802841186523) q[6];\n",
      "rz(10.583752632141113) q[6];\n",
      "rx(6.51846170425415) q[6];\n",
      "crz(1.4483122825622559) q[6], q[5];\n",
      "rx(7.855419635772705) q[0];\n",
      "rz(11.047640800476074) q[0];\n",
      "rx(3.675446033477783) q[0];\n",
      "rx(10.516045570373535) q[4];\n",
      "rz(6.381658554077148) q[4];\n",
      "rx(12.414477348327637) q[4];\n",
      "crz(2.405231237411499) q[4], q[0];\n",
      "rx(3.5967392921447754) q[0];\n",
      "rz(10.639686584472656) q[0];\n",
      "rx(2.20661997795105) q[0];\n",
      "rx(8.454360008239746) q[4];\n",
      "rz(1.722045660018921) q[4];\n",
      "rx(1.1698485612869263) q[4];\n",
      "crz(6.6205854415893555) q[0], q[4];\n",
      "rx(4.704521179199219) q[1];\n",
      "rz(3.956341505050659) q[1];\n",
      "rx(9.28282356262207) q[1];\n",
      "rx(8.575423240661621) q[5];\n",
      "rz(10.197956085205078) q[5];\n",
      "rx(11.179265022277832) q[5];\n",
      "crz(7.509900093078613) q[5], q[1];\n",
      "rx(6.448909282684326) q[1];\n",
      "rz(4.889273166656494) q[1];\n",
      "rx(4.587276458740234) q[1];\n",
      "rx(4.1869001388549805) q[5];\n",
      "rz(6.924800395965576) q[5];\n",
      "rx(2.910736560821533) q[5];\n",
      "crz(7.926799297332764) q[1], q[5];\n",
      "rx(4.379952907562256) q[2];\n",
      "rz(11.244744300842285) q[2];\n",
      "rx(12.01327133178711) q[2];\n",
      "rx(7.885587692260742) q[6];\n",
      "rz(7.447887420654297) q[6];\n",
      "rx(10.245769500732422) q[6];\n",
      "crz(7.297436714172363) q[6], q[2];\n",
      "rx(8.584932327270508) q[2];\n",
      "rz(2.5691299438476562) q[2];\n",
      "rx(6.714617729187012) q[2];\n",
      "rx(11.254437446594238) q[6];\n",
      "rz(10.866512298583984) q[6];\n",
      "rx(8.502589225769043) q[6];\n",
      "crz(2.3050737380981445) q[2], q[6];\n",
      "rx(5.306732654571533) q[3];\n",
      "rz(1.985135555267334) q[3];\n",
      "rx(7.002983570098877) q[3];\n",
      "rx(11.751270294189453) q[7];\n",
      "rz(6.761312484741211) q[7];\n",
      "rx(10.752816200256348) q[7];\n",
      "crz(12.48620891571045) q[7], q[3];\n",
      "rx(0.014510774984955788) q[3];\n",
      "rz(9.194474220275879) q[3];\n",
      "rx(2.4289276599884033) q[3];\n",
      "rx(1.3681684732437134) q[7];\n",
      "rz(9.949748992919922) q[7];\n",
      "rx(12.3832426071167) q[7];\n",
      "crz(4.438501358032227) q[3], q[7];\n",
      "rx(2.7538845539093018) q[4];\n",
      "rz(7.4493489265441895) q[4];\n",
      "rx(1.9669426679611206) q[4];\n",
      "rx(6.124652862548828) q[5];\n",
      "rz(7.487969875335693) q[5];\n",
      "rx(0.6880599856376648) q[5];\n",
      "crz(2.4400274753570557) q[4], q[5];\n",
      "rx(1.4344666004180908) q[6];\n",
      "rz(8.332429885864258) q[6];\n",
      "rx(11.17200756072998) q[6];\n",
      "rx(9.338862419128418) q[7];\n",
      "rz(8.766731262207031) q[7];\n",
      "rx(2.4631216526031494) q[7];\n",
      "crz(9.86589241027832) q[6], q[7];\n",
      "rx(-0.17794841527938843) q[5];\n",
      "rz(4.332483291625977) q[5];\n",
      "rx(9.894854545593262) q[5];\n",
      "rx(3.169703960418701) q[6];\n",
      "rz(4.50349235534668) q[6];\n",
      "rx(1.0442160367965698) q[6];\n",
      "crz(1.8257908821105957) q[6], q[5];\n",
      "rx(9.344486236572266) q[4];\n",
      "rz(10.42884349822998) q[4];\n",
      "rx(2.7318480014801025) q[4];\n",
      "rx(12.485259056091309) q[6];\n",
      "rz(8.48313045501709) q[6];\n",
      "rx(3.7595596313476562) q[6];\n",
      "crz(9.383272171020508) q[6], q[4];\n",
      "rx(3.8164587020874023) q[4];\n",
      "rz(9.709908485412598) q[4];\n",
      "rx(0.602480411529541) q[4];\n",
      "rx(4.813307285308838) q[6];\n",
      "rz(4.310812950134277) q[6];\n",
      "rx(12.457308769226074) q[6];\n",
      "crz(5.916712760925293) q[4], q[6];\n",
      "rx(3.9488637447357178) q[5];\n",
      "rz(13.032925605773926) q[5];\n",
      "rx(8.248537063598633) q[5];\n",
      "rx(7.782092571258545) q[7];\n",
      "rz(10.5662841796875) q[7];\n",
      "rx(9.485393524169922) q[7];\n",
      "crz(1.6305420398712158) q[7], q[5];\n",
      "rx(5.165726661682129) q[5];\n",
      "rz(9.26846694946289) q[5];\n",
      "rx(2.976287841796875) q[5];\n",
      "rx(7.585885047912598) q[7];\n",
      "rz(11.967287063598633) q[7];\n",
      "rx(7.808928489685059) q[7];\n",
      "crz(9.847161293029785) q[5], q[7];\n",
      "rx(4.100030422210693) q[7];\n",
      "rz(8.498974800109863) q[7];\n",
      "rx(1.211074948310852) q[7];\n",
      "measure q[7] -> c[0];\n"
     ]
    }
   ],
   "source": [
    "print(model.to_qasm())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
